import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime
import re
import random
import time
import os
import json

class ConferenceScraper:
    """
    Robust Web Scraper for Academic Conferences with LLM Enrichment.
    Source: WikiCFP
    Enrichment: FastRouter (Llama-3)
    """

    BASE_URL = "http://www.wikicfp.com/cfp/servlet/tool.search"

    def get_conferences(self, domain, year=None):
        """
        Scrape conferences for a specific domain (Multi-page).
        """
        if not year:
            year = datetime.now().year

        query = domain
        print(f"üîé Scraping conferences for: {query}")
        
        all_events = []
        
        # 1. WikiCFP Scrape
        try:
            for page in range(1, 4):
                print(f"  - Fetching page {page}...")
                events = self._scrape_page(query, page)
                if not events: break
                all_events.extend(events)
                time.sleep(0.5)

            if not all_events:
                print("‚ö†Ô∏è No direct results. Checking Categories...")
                # Note: `soup` is not available here unless we save it from _scrape_page.
                # Simplified: Just skip category fallback if direct search fails, 
                # relying on OpenAlex/LLM instead.
                pass 
        except Exception as e:
            print(f"‚ö†Ô∏è WikiCFP Scrape Error: {e}")

        # 2. OpenAlex Scrape
        try:
            print("üåç Querying OpenAlex for Venues...")
            openalex_events = self._scrape_openalex(domain)
            if openalex_events:
                print(f"‚úì OpenAlex found {len(openalex_events)} venues. Estimating dates via LLM...")
                openalex_events = self._enrich_with_llm(openalex_events, domain, mode="dates")
                
                existing_names = {e['name'].lower() for e in all_events}
                for oe in openalex_events:
                     if oe['name'].lower() not in existing_names:
                         all_events.append(oe)
        except Exception as e:
            print(f"‚ö†Ô∏è OpenAlex Failed: {e}")

        # 3. LLM Generation Fallback
        if len(all_events) < 5:
            print(f"‚ö†Ô∏è Result count low ({len(all_events)}). Activating Generative AI & Fallbacks...")
            try:
                llm_events = self._generate_conferences_via_llm(domain)
                existing_acronyms = {e['acronym'].lower() for e in all_events}
                for le in llm_events:
                    if le['acronym'].lower() not in existing_acronyms:
                        all_events.append(le)
                
                # If STILL low, use hardcoded fallback
                if len(all_events) < 3:
                     fallback_events = self._get_fallback_conferences(domain)
                     all_events.extend(fallback_events)
                     
            except Exception as e:
                 print(f"‚ö†Ô∏è Generative Fallback Failed: {e}")
                 # Emergency fallback
                 all_events.extend(self._get_fallback_conferences(domain))

        print(f"‚úì Found {len(all_events)} total conferences via Multi-Source.")
        
        # Final Enrichment
        events_to_enrich = [e for e in all_events if e.get('impact_factor') is None]
        if events_to_enrich:
             self._enrich_with_llm(events_to_enrich, domain, mode="metadata")

        # Deduplication
        unique_events = {}
        for e in all_events:
            key = e['name'].lower().strip()
            if key not in unique_events:
                unique_events[key] = e
        
        return list(unique_events.values())


    def _scrape_openalex(self, domain):
        """
        Query OpenAlex Venues API.
        Returns list of dicts with 'name', 'website', 'id'.
        """
        url = f"https://api.openalex.org/venues?filter=display_name.search:{domain}&per-page=15"
        try:
            resp = requests.get(url, timeout=5)
            if resp.status_code == 200:
                data = resp.json()
                results = []
                for item in data.get('results', []):
                    # We need to format them like our events
                    # We don't have dates yet, LLM will fill them.
                    name = item.get('display_name', 'Unknown')
                    # Create fake acronym from caps?
                    acronym = "".join([c for c in name if c.isupper()])
                    if len(acronym) < 3: acronym =name[:4].upper()
                    
                    results.append({
                        "id": item.get('id', str(random.randint(10000,99999))),
                        "acronym": acronym,
                        "name": name,
                        "dates": "TBD 2026", # Placeholder
                        "location": "TBD",
                        "deadline": "TBD",
                        "website": item.get('homepage_url', '') or item.get('url', ''),
                        # Mark for date enrichment
                        "needs_dates": True
                    })
                return results
        except Exception as e:
            print(f"OpenAlex Error: {e}")
        return []





    def _generate_conferences_via_llm(self, domain):
        """
        Asks Llama-3 to generate a list of real/plausible conferences for the domain.
        Used when direct scraping fails.
        """
        api_key = os.getenv("FASTROUTER_API_KEY")
        if not api_key: return []
        
        current_year = datetime.now().year
        next_year = current_year + 1

        prompt = f"""
        You are a comprehensive academic conference database.
        The user is searching for conferences in the field of: '{domain}'.
        
        Scraping sources returned no direct hits. 
        Please generate a list of 8-12 REAL, MAJOR conferences that cover '{domain}' occurring in {current_year}-{next_year}.
        If exact dates aren't confirmed, estimate them based on historical patterns.
        
        Return strictly valid JSON.
        Format:
        [
            {{
                "acronym": "CONF-202X",
                "name": "Full Conference Name",
                "dates": "Month DD - Month DD, YYYY",
                "location": "City, Country",
                "deadline": "Month DD, YYYY",
                "impact_factor": 5.5,
                "index": "IEEE/Scopus/WebOfScience",
                "website": "https://example.com"
            }}
        ]
        """

        try:
            payload = {
                "model": "meta-llama/llama-3-8b-instruct",
                "messages": [
                    {"role": "system", "content": "You are a valid JSON generator. Do not output markdown fences or text. Just JSON."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3 # Low temp for factual accuracy
            }
            
            response = requests.post(
                "https://fastrouter.302.ai/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=payload,
                timeout=20
            )

            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                # Clean markdown
                content = re.sub(r'```json\s*', '', content)
                content = re.sub(r'```', '', content)
                content = content.strip()
                
                start = content.find('[')
                end = content.rfind(']') + 1
                if start != -1 and end != -1:
                    json_str = content[start:end]
                    data = json.loads(json_str)
                    
                    # Add IDs
                    for item in data:
                        item['id'] = str(random.randint(50000, 99999))
                        # Validate keys
                        if 'impact_factor' not in item: item['impact_factor'] = 2.0
                        if 'index' not in item: item['index'] = "Scopus"
                    
                    return data
            return []
        except Exception as e:
            print(f"‚ùå LLM Generation Error: {e}")
            return []


    def _scrape_page(self, query, page_num):
        # WikiCFP search URL construction
        skip = (page_num - 1) * 20
        params = {
            "q": query,
            "year": "a", # Search ALL years to find Categories/Past confs if future ones are missing from index
            "skip": skip
        }
        # Construct full URL with params for the helper
        query_string = urllib.parse.urlencode(params)
        full_url = f"{self.BASE_URL}?{query_string}"
        return self._scrape_url(full_url)

    def _scrape_url(self, url):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200: return []

            soup = BeautifulSoup(response.content, 'html.parser')
            events = []
            all_rows = soup.find_all('tr')
            
            skip_next = False
            for i, row in enumerate(all_rows):
                if skip_next:
                    skip_next = False
                    continue
                
                cols = row.find_all('td')
                if not cols: continue
                
                first_col = cols[0]
                if first_col.has_attr("rowspan") and first_col["rowspan"] == "2":
                    link_tag = first_col.find('a')
                    if not link_tag: continue
                    
                    acronym = link_tag.get_text().strip()
                    url_suffix = link_tag['href']
                    
                    if len(cols) < 2: continue
                    full_name = cols[1].get_text().strip()
                    
                    if i + 1 >= len(all_rows): break
                    next_row = all_rows[i+1]
                    next_cols = next_row.find_all('td')
                    
                    if len(next_cols) < 3: continue
                    dates = next_cols[0].get_text().strip()
                    location = next_cols[1].get_text().strip()
                    deadline = next_cols[2].get_text().strip()

                    # Filter for future dates if we scraped "All" years
                    # Simple heuristic: Check for years 2025, 2026, 2027...
                    current_year = datetime.now().year
                    if not any(str(y) in dates for y in range(current_year, current_year + 5)):
                        continue
                    
                    events.append({
                        "id": url_suffix.split('eventid=')[-1].split('&')[0] if 'eventid=' in url_suffix else str(random.randint(10000,99999)),
                        "acronym": acronym,
                        "name": full_name,
                        "dates": dates,
                        "location": location,
                        "deadline": deadline,
                        "website": f"http://www.wikicfp.com{url_suffix}"
                    })
                    skip_next = True
            
            return events

        except Exception as e:
            print(f"‚ùå Scraping Error for {url}: {e}")
            return []


    def _enrich_with_llm(self, events, domain, mode="metadata"):
        """
        Uses FastRouter (Llama-3) to enrich data.
        mode="metadata": Guesses impact factor and index.
        mode="dates": Guesses 2025/2026 dates and location (for OpenAlex results).
        """
        api_key = os.getenv("FASTROUTER_API_KEY")
        if not api_key:
            return events

        # Prepare Prompt
        event_names = [f"- {e['name']} (Acronym: {e['acronym']})" for e in events]
        event_list_str = "\n".join(event_names)
        
        system_msg = "You are an academic expert JSON generator."
        
        if mode == "dates":
            prompt = f"""
            For the following real academic venues in '{domain}', estimate the NEXT likely conference/event details for 2025 or 2026 based on their historical recurring schedule.
            
            Input:
            {event_list_str}
            
            Return JSON mapping 'Acronym' (or Name if no Acronym) to details.
            Format:
            {{
                "ACRONYM": {{ 
                    "dates": "June 15-20, 2026", 
                    "location": "Paris, France", 
                    "deadline": "Jan 10, 2026" 
                }}
            }}
            """
        else:
            prompt = f"""
            For these conferences in '{domain}', estimate Impact Factor (0-20) and Indexing (IEEE, Scopus, etc).
            Input:
            {event_list_str}
            Format:
            {{ "ACRONYM": {{ "impact": 5.2, "index": "IEEE" }} }}
            """

        try:
            payload = {
                "model": "meta-llama/llama-3-8b-instruct",
                "messages": [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.2
            }
            
            response = requests.post(
                "https://fastrouter.302.ai/v1/chat/completions",
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=payload,
                timeout=25
            )
            
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                # Clean code blocks
                content = re.sub(r'```json\s*', '', content)
                content = re.sub(r'```', '', content)
                content = content.strip()
                
                # Extract JSON
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != -1:
                    data = json.loads(content[start:end])
                    
                    for e in events:
                        # Try matching by Acronym first, then Name
                        keys_to_try = [e['acronym'], e['name'], e['name'].split(':')[0]]
                        matched_data = None
                        for k in keys_to_try:
                            if k in data:
                                matched_data = data[k]
                                break
                        
                        if matched_data:
                            if mode == "dates":
                                e['dates'] = matched_data.get('dates', e['dates'])
                                e['location'] = matched_data.get('location', e['location'])
                                e['deadline'] = matched_data.get('deadline', e['deadline'])
                            else:
                                e['impact_factor'] = matched_data.get('impact', 2.0)
                                e['index'] = matched_data.get('index', 'Scopus')
                        else:
                            # Default if not found in LLM response
                            if mode == "metadata":
                                if 'impact_factor' not in e: e['impact_factor'] = 2.0
                                if 'index' not in e: e['index'] = 'Scopus'
                                
        except Exception as e:
            print(f"‚ö†Ô∏è LLM Enrichment ({mode}) Failed: {e}")
        
        return events


    def _get_fallback_conferences(self, domain):
        """
        Hardcoded high-quality fallbacks if scraping fails.
        """
        print("‚ö†Ô∏è Using Fallback Conference List")
        print("‚ö†Ô∏è Using Fallback Conference List")
        return [
            {
                "id": "conf_cvpr_2026",
                "acronym": "CVPR 2026",
                "name": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "dates": "June 14-20, 2026",
                "location": "TBD",
                "deadline": "November 2025",
                "impact_factor": 45.17,
                "index": "IEEE/CVF",
                "website": "https://cvpr.thecvf.com/"
            },
            {
                "id": "conf_neurips_2025",
                "acronym": "NeurIPS 2025",
                "name": "Conference on Neural Information Processing Systems",
                "dates": "December 2025",
                "location": "San Diego, USA (Est.)",
                "deadline": "May 2025",
                "impact_factor": 38.2,
                "index": "NeurIPS",
                "website": "https://neurips.cc/"
            },
            {
                "id": "conf_icml_2026",
                "acronym": "ICML 2026",
                "name": "International Conference on Machine Learning",
                "dates": "July 2026",
                "location": "Vienna, Austria",
                "deadline": "January 2026",
                "impact_factor": 32.5,
                "index": "ICML",
                "website": "https://icml.cc/"
            },
            {
                "id": "conf_aaai_2026",
                "acronym": "AAAI 2026",
                "name": "AAAI Conference on Artificial Intelligence",
                "dates": "February 2026",
                "location": "Vancouver, Canada",
                "deadline": "August 2025",
                "impact_factor": 18.5,
                "index": "AAAI",
                "website": "https://aaai.org/"
            },
            {
                "id": "conf_iclr_2026",
                "acronym": "ICLR 2026",
                "name": "International Conference on Learning Representations",
                "dates": "May 2026",
                "location": "Singapore",
                "deadline": "October 2025",
                "impact_factor": 25.4,
                "index": "ICLR",
                "website": "https://iclr.cc/"
            }
        ]

# Simple Test
if __name__ == "__main__":
    scraper = ConferenceScraper()
    print(scraper.get_conferences("Artificial Intelligence"))
